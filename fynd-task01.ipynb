{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-05T23:57:23.926001Z",
     "iopub.status.busy": "2025-12-05T23:57:23.925661Z",
     "iopub.status.idle": "2025-12-05T23:57:23.997457Z",
     "shell.execute_reply": "2025-12-05T23:57:23.996394Z",
     "shell.execute_reply.started": "2025-12-05T23:57:23.925978Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Setup complete. OpenRouter Client initialized using stable model: mistralai/mistral-7b-instruct\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import warnings\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "# 1. Set the OpenRouter API Key and Base URL\n",
    "OPENROUTER_API_KEY = \"Configure your OpenRouter API key here\"\n",
    "OPENROUTER_BASE_URL = \"https://openrouter.ai/api/v1\"\n",
    "\n",
    "# 2. Define the OPENROUTER Client\n",
    "client = OpenAI(\n",
    "    base_url=OPENROUTER_BASE_URL,\n",
    "    api_key=OPENROUTER_API_KEY\n",
    ")\n",
    "\n",
    "# 3. USE MISTRAL 7B WITHOUT THE ':free' SUFFIX\n",
    "# This should work reliably once your quota/credits are available.\n",
    "LLM_MODEL = \"mistralai/mistral-7b-instruct\" \n",
    "\n",
    "print(f\"‚úÖ Setup complete. OpenRouter Client initialized using stable model: {LLM_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T23:57:23.999363Z",
     "iopub.status.busy": "2025-12-05T23:57:23.999118Z",
     "iopub.status.idle": "2025-12-05T23:57:24.118155Z",
     "shell.execute_reply": "2025-12-05T23:57:24.117072Z",
     "shell.execute_reply.started": "2025-12-05T23:57:23.999344Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load data at: /kaggle/input/yelp-file/yelp.csv...\n",
      "  -> Trying delimiter: ','\n",
      "‚úÖ Data loaded successfully using delimiter: ','\n",
      "Total rows: 10000 | Sampled rows: 200\n",
      "\n",
      "Sampled Data Head (Ready for prediction):\n",
      "                                                text  actual_stars\n",
      "0  We got here around midnight last Friday... the...             4\n",
      "1  Brought a friend from Louisiana here.  She say...             5\n",
      "2  Every friday, my dad and I eat here. We order ...             3\n",
      "3  My husband and I were really, really disappoin...             1\n",
      "4  Love this place!  Was in phoenix 3 weeks for w...             5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# ‚úÖ CONFIRMED CORRECT FILE PATH from the diagnostic:\n",
    "FILE_PATH = '/kaggle/input/yelp-file/yelp.csv' \n",
    "SAMPLE_SIZE = 200\n",
    "\n",
    "df_sample = pd.DataFrame() \n",
    "\n",
    "def load_data_robustly(file_path):\n",
    "    \"\"\"Attempts to load data using comma, then tab, then pipe, then semicolon delimiters.\"\"\"\n",
    "    delimiters_to_try = [',', '\\t', '|', ';']\n",
    "    df = None\n",
    "    \n",
    "    print(f\"Attempting to load data at: {file_path}...\")\n",
    "\n",
    "    for sep in delimiters_to_try:\n",
    "        try:\n",
    "            print(f\"  -> Trying delimiter: '{sep}'\")\n",
    "            # Using on_bad_lines='skip' handles corrupt lines\n",
    "            df = pd.read_csv(file_path, sep=sep, on_bad_lines='skip', encoding='utf-8')\n",
    "            \n",
    "            # Check for reasonable column count and rows\n",
    "            if len(df.columns) > 1 and not df.empty:\n",
    "                return df, sep\n",
    "            \n",
    "        except Exception:\n",
    "            pass # Silently continue if loading fails\n",
    "\n",
    "    return None, None\n",
    "\n",
    "try:\n",
    "    df_full, successful_sep = load_data_robustly(FILE_PATH)\n",
    "    \n",
    "    if df_full is None:\n",
    "        raise ValueError(\"Failed to load DataFrame with all attempted delimiters. Check file integrity.\")\n",
    "\n",
    "    print(f\"‚úÖ Data loaded successfully using delimiter: '{successful_sep}'\")\n",
    "    \n",
    "    # Check for required columns ('text' and 'stars' are the standard names)\n",
    "    if 'text' in df_full.columns and 'stars' in df_full.columns:\n",
    "        # Sample and rename the columns for the prediction phase\n",
    "        df_sample = df_full[['text', 'stars']].sample(n=SAMPLE_SIZE, random_state=42).reset_index(drop=True)\n",
    "        df_sample.rename(columns={'stars': 'actual_stars'}, inplace=True)\n",
    "        \n",
    "        print(f\"Total rows: {len(df_full)} | Sampled rows: {len(df_sample)}\")\n",
    "        print(\"\\nSampled Data Head (Ready for prediction):\")\n",
    "        print(df_sample.head())\n",
    "\n",
    "    else:\n",
    "        print(\"‚ùå ERROR: Required columns ('text' and 'stars') not found after loading.\")\n",
    "        print(\"Available columns:\", df_full.columns.tolist())\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå FATAL ERROR: Data loading failed. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T23:57:24.119357Z",
     "iopub.status.busy": "2025-12-05T23:57:24.119064Z",
     "iopub.status.idle": "2025-12-05T23:57:24.238342Z",
     "shell.execute_reply": "2025-12-05T23:57:24.237416Z",
     "shell.execute_reply.started": "2025-12-05T23:57:24.119327Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load data at: /kaggle/input/yelp-file/yelp.csv...\n",
      "  -> Trying delimiter: ','\n",
      "‚úÖ Data loaded successfully using delimiter: ','\n",
      "Total rows: 10000 | Sampled rows: 200\n",
      "\n",
      "Sampled Data Head (Ready for prediction):\n",
      "                                                text  actual_stars\n",
      "0  We got here around midnight last Friday... the...             4\n",
      "1  Brought a friend from Louisiana here.  She say...             5\n",
      "2  Every friday, my dad and I eat here. We order ...             3\n",
      "3  My husband and I were really, really disappoin...             1\n",
      "4  Love this place!  Was in phoenix 3 weeks for w...             5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# ‚úÖ CONFIRMED CORRECT FILE PATH:\n",
    "FILE_PATH = '/kaggle/input/yelp-file/yelp.csv' \n",
    "SAMPLE_SIZE = 200\n",
    "\n",
    "df_sample = pd.DataFrame() \n",
    "\n",
    "def load_data_robustly(file_path):\n",
    "    \"\"\"Attempts to load data using comma, then tab, then pipe, then semicolon delimiters.\"\"\"\n",
    "    delimiters_to_try = [',', '\\t', '|', ';'] # Added semicolon\n",
    "    df = None\n",
    "    \n",
    "    print(f\"Attempting to load data at: {file_path}...\")\n",
    "\n",
    "    for sep in delimiters_to_try:\n",
    "        try:\n",
    "            print(f\"  -> Trying delimiter: '{sep}'\")\n",
    "            # Using on_bad_lines='skip' handles corrupt lines\n",
    "            df = pd.read_csv(file_path, sep=sep, on_bad_lines='skip', encoding='utf-8')\n",
    "            \n",
    "            # Check for reasonable column count and rows\n",
    "            if len(df.columns) > 1 and not df.empty:\n",
    "                return df, sep\n",
    "            \n",
    "        except Exception:\n",
    "            pass # Silently continue if loading fails\n",
    "\n",
    "    return None, None\n",
    "\n",
    "try:\n",
    "    df_full, successful_sep = load_data_robustly(FILE_PATH)\n",
    "    \n",
    "    if df_full is None:\n",
    "        raise ValueError(\"Failed to load DataFrame with all attempted delimiters. Please check file integrity.\")\n",
    "\n",
    "    print(f\"‚úÖ Data loaded successfully using delimiter: '{successful_sep}'\")\n",
    "    \n",
    "    # Check for required columns ('text' and 'stars' are the standard names)\n",
    "    if 'text' in df_full.columns and 'stars' in df_full.columns:\n",
    "        # Sample and rename the columns for the prediction phase\n",
    "        df_sample = df_full[['text', 'stars']].sample(n=SAMPLE_SIZE, random_state=42).reset_index(drop=True)\n",
    "        df_sample.rename(columns={'stars': 'actual_stars'}, inplace=True)\n",
    "        \n",
    "        print(f\"Total rows: {len(df_full)} | Sampled rows: {len(df_sample)}\")\n",
    "        print(\"\\nSampled Data Head (Ready for prediction):\")\n",
    "        print(df_sample.head())\n",
    "\n",
    "    else:\n",
    "        # This handles the case where the data loaded but the columns are named differently (e.g., 'Review' and 'Rating')\n",
    "        print(\"‚ùå ERROR: Required columns ('text' and 'stars') not found after loading.\")\n",
    "        print(\"Available columns:\", df_full.columns.tolist())\n",
    "        print(\"You must rename the correct columns (e.g., df_full.rename(columns={'Review': 'text', 'Rating': 'stars'}, inplace=True))\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå FATAL ERROR: Data loading failed. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T23:57:24.241117Z",
     "iopub.status.busy": "2025-12-05T23:57:24.240769Z",
     "iopub.status.idle": "2025-12-05T23:57:24.250142Z",
     "shell.execute_reply": "2025-12-05T23:57:24.249083Z",
     "shell.execute_reply.started": "2025-12-05T23:57:24.241097Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 2. Main Processing Function (OpenRouter Implementation) ---\n",
    "def predict_rating(review_text, prompt_function, strategy_name):\n",
    "    \"\"\"Sends a single review to the OpenRouter API and processes the response.\"\"\"\n",
    "    \n",
    "    prompt = prompt_function(review_text)\n",
    "    \n",
    "    try:\n",
    "        # Use ChatCompletion with strict JSON mode enabled\n",
    "        response = client.chat.completions.create(\n",
    "            model=LLM_MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a Yelp review rating model. Respond strictly in the required JSON format.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            temperature=0.0\n",
    "        )\n",
    "\n",
    "        json_str = response.choices[0].message.content.strip()\n",
    "\n",
    "        try:\n",
    "            result = json.loads(json_str)\n",
    "            \n",
    "            # üö® FIX: Model outputs 'rating', so we check for both 'rating' and 'predicted_stars'\n",
    "            pred_stars = result.get('rating') or result.get('predicted_stars')\n",
    "            explanation = result.get('explanation', '')\n",
    "            \n",
    "            # If the model included the review text, extract the rating from that output.\n",
    "            if pred_stars is None and 'review' in result and 'rating' in result:\n",
    "                 pred_stars = result.get('rating') \n",
    "\n",
    "\n",
    "            if isinstance(pred_stars, int) and 1 <= pred_stars <= 5:\n",
    "                return {\n",
    "                    'predicted_stars': pred_stars,\n",
    "                    'explanation': explanation,\n",
    "                    'json_valid': True\n",
    "                }\n",
    "            else:\n",
    "                # If the key was found but the value was not an int 1-5\n",
    "                return {\n",
    "                    'predicted_stars': np.nan,\n",
    "                    'explanation': json_str,\n",
    "                    'json_valid': True\n",
    "                }\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            # If the output wasn't a valid JSON\n",
    "            return {\n",
    "                'predicted_stars': np.nan,\n",
    "                'explanation': json_str,\n",
    "                'json_valid': False\n",
    "            }\n",
    "\n",
    "    except Exception as e:\n",
    "        # API Error (Rate Limit 429, etc.)\n",
    "        return {\n",
    "            'predicted_stars': np.nan,\n",
    "            'explanation': f\"API_ERROR: {e}\",\n",
    "            'json_valid': False\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T23:57:24.251291Z",
     "iopub.status.busy": "2025-12-05T23:57:24.251037Z",
     "iopub.status.idle": "2025-12-06T00:25:05.797904Z",
     "shell.execute_reply": "2025-12-06T00:25:05.796370Z",
     "shell.execute_reply.started": "2025-12-05T23:57:24.251272Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Rating Prediction for 200 reviews ---\n",
      "\n",
      "Processing Strategy: **P1_Base_Prompt**\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   P1_Base_Prompt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [07:42<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Strategy: **P2_CoT_FewShot**\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   P2_CoT_FewShot: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [15:23<00:00,  4.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Strategy: **P3_Persona_AntiExamples**\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   P3_Persona_AntiExamples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [04:35<00:00,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- All predictions complete ---\n",
      "\n",
      "Sample of results_df (Actual vs Predictions):\n",
      "   actual_stars  P1_Base_Prompt_pred  P2_CoT_FewShot_pred  \\\n",
      "0             4                    4                    4   \n",
      "1             5                    5                    5   \n",
      "2             3                    3                    4   \n",
      "3             1                    1                    1   \n",
      "4             5                    5                    5   \n",
      "\n",
      "   P3_Persona_AntiExamples_pred  \n",
      "0                             4  \n",
      "1                             5  \n",
      "2                             4  \n",
      "3                             1  \n",
      "4                             5  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Define Strategies to Run ---\n",
    "strategies = [\n",
    "    {'name': 'P1_Base_Prompt', 'func': get_prompt_p1},\n",
    "    {'name': 'P2_CoT_FewShot', 'func': get_prompt_p2},\n",
    "    {'name': 'P3_Persona_AntiExamples', 'func': get_prompt_p3},\n",
    "]\n",
    "\n",
    "# Create a DataFrame to store all results (copies df_sample from Cell 2)\n",
    "# This assumes df_sample was successfully created in the previous step.\n",
    "results_df = df_sample.copy()\n",
    "SAMPLE_SIZE = len(results_df)\n",
    "\n",
    "print(f\"--- Starting Rating Prediction for {SAMPLE_SIZE} reviews ---\")\n",
    "\n",
    "# --- RATE LIMITING CONFIGURATION ---\n",
    "DELAY_SECONDS = 0.5 \n",
    "REQUESTS_BEFORE_DELAY = 10 \n",
    "# -------------------------------------\n",
    "\n",
    "# Iterate through each prompting strategy\n",
    "for strategy in strategies:\n",
    "    strategy_name = strategy['name']\n",
    "    prompt_func = strategy['func']\n",
    "\n",
    "    print(f\"\\nProcessing Strategy: **{strategy_name}**\")\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    # Use tqdm for a progress bar while iterating through the reviews\n",
    "    for i, review_text in enumerate(tqdm(results_df['text'], desc=f\"   {strategy_name}\")):\n",
    "\n",
    "        # Implement Delay Check\n",
    "        if i > 0 and i % REQUESTS_BEFORE_DELAY == 0:\n",
    "            time.sleep(DELAY_SECONDS)\n",
    "\n",
    "        # Get prediction and metadata\n",
    "        result = predict_rating(review_text, prompt_func, strategy_name)\n",
    "        predictions.append(result)\n",
    "\n",
    "    # Convert results to a temporary DataFrame and add to main DataFrame\n",
    "    temp_df = pd.DataFrame(predictions)\n",
    "    results_df[f'{strategy_name}_pred'] = temp_df['predicted_stars'].values\n",
    "    results_df[f'{strategy_name}_valid'] = temp_df['json_valid'].values\n",
    "    results_df[f'{strategy_name}_explanation'] = temp_df['explanation'].values\n",
    "\n",
    "\n",
    "print(\"\\n--- All predictions complete ---\")\n",
    "print(\"\\nSample of results_df (Actual vs Predictions):\")\n",
    "print(results_df[[\n",
    "    'actual_stars', 'P1_Base_Prompt_pred', 'P2_CoT_FewShot_pred', 'P3_Persona_AntiExamples_pred'\n",
    "]].head())\n",
    "\n",
    "# --- Evaluation ---\n",
    "evaluation_metrics = []\n",
    "\n",
    "for strategy in strategies:\n",
    "    strategy_name = strategy['name']\n",
    "    pred_col = f'{strategy_name}_pred'\n",
    "    \n",
    "    # Calculate Accuracy (on valid predictions)\n",
    "    correct_predictions = np.sum(np.isclose(results_df['actual_stars'], results_df[pred_col]))\n",
    "    valid_pred_count = results_df[pred_col].count()\n",
    "    accuracy = correct_predictions / valid_pred_count if valid_pred_count > 0 else 0\n",
    "    \n",
    "    # Calculate JSON Validity Rate\n",
    "    json_valid_count = results_df[f'{strategy_name}_valid'].sum()\n",
    "    validity_rate = json_valid_count / SAMPLE_SIZE\n",
    "    \n",
    "    # Calculate Reliability (Standard Deviation of Absolute Error)\n",
    "    error = np.abs(results_df['actual_stars'] - results_df[pred_col])\n",
    "    consistency_std = error[~error.isna()].std()\n",
    "    \n",
    "    evaluation_metrics.append({\n",
    "        'Approach': strategy_name,\n",
    "        'Total Valid Predictions': valid_pred_count,\n",
    "        'Accuracy (on Valid Predictions)': accuracy,\n",
    "        'JSON Validity Rate': validity_rate,\n",
    "        'Reliability (Std Dev of Error)': consistency_std\n",
    "    })\n",
    "\n",
    "# Convert metrics to a final comparison table\n",
    "comparison_df = pd.DataFrame(evaluation_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T00:25:20.028644Z",
     "iopub.status.busy": "2025-12-06T00:25:20.028101Z",
     "iopub.status.idle": "2025-12-06T00:25:20.043339Z",
     "shell.execute_reply": "2025-12-06T00:25:20.042308Z",
     "shell.execute_reply.started": "2025-12-06T00:25:20.028600Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "# üìä Evaluation Results: Prompting Strategy Comparison\n",
      "\n",
      "--- Performance Metrics Table ---\n",
      "\n",
      "--- Diagnostic Check (First Invalid Responses) ---\n",
      "P2 diagnostic check: All predictions were valid (1-5 integer).\n"
     ]
    }
   ],
   "source": [
    "# --- RUN THIS CODE NOW (Cell 5) ---\n",
    "from IPython.display import display\n",
    "\n",
    "print(\"\\n\\n# üìä Evaluation Results: Prompting Strategy Comparison\\n\")\n",
    "\n",
    "# Display the main comparison table\n",
    "print(\"--- Performance Metrics Table ---\")\n",
    "# The comparison_df creation failed because all predictions were NaN, but let's try the diagnostic check.\n",
    "    \n",
    "print(\"\\n--- Diagnostic Check (First Invalid Responses) ---\")\n",
    "# Check the raw output for the first failure in P2 (as an example)\n",
    "try:\n",
    "    # We explicitly check for the NaN values in the prediction column\n",
    "    invalid_p2 = results_df[results_df['P2_CoT_FewShot_pred'].isna()].head(1)\n",
    "    \n",
    "    if not invalid_p2.empty:\n",
    "        print(\"\\nExample P2 Failure (Raw Model Output):\")\n",
    "        # Find the index of the first failure for clear display\n",
    "        for index, row in invalid_p2.iterrows():\n",
    "            print(f\"Actual Star Rating: {row['actual_stars']}\")\n",
    "            print(\"----------------------------------------\")\n",
    "            # This is the key: the raw model output that caused the NaN\n",
    "            print(f\"RAW MODEL OUTPUT: {row['P2_CoT_FewShot_explanation']}\")\n",
    "    else:\n",
    "        print(\"P2 diagnostic check: All predictions were valid (1-5 integer).\")\n",
    "        \n",
    "except NameError:\n",
    "    print(\"‚ùå ERROR: 'results_df' is not defined. Please ensure Cell 4 completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T00:31:14.071337Z",
     "iopub.status.busy": "2025-12-06T00:31:14.071020Z",
     "iopub.status.idle": "2025-12-06T00:31:14.084968Z",
     "shell.execute_reply": "2025-12-06T00:31:14.083250Z",
     "shell.execute_reply.started": "2025-12-06T00:31:14.071312Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RAW RESULTS DUMP ---\n",
      "Comparison Metrics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Approach</th>\n",
       "      <th>Total Valid Predictions</th>\n",
       "      <th>Accuracy (on Valid Predictions)</th>\n",
       "      <th>JSON Validity Rate</th>\n",
       "      <th>Reliability (Std Dev of Error)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P1_Base_Prompt</td>\n",
       "      <td>200</td>\n",
       "      <td>0.640</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.515464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P2_CoT_FewShot</td>\n",
       "      <td>200</td>\n",
       "      <td>0.675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.505647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P3_Persona_AntiExamples</td>\n",
       "      <td>200</td>\n",
       "      <td>0.630</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.549097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Approach  Total Valid Predictions  \\\n",
       "0           P1_Base_Prompt                      200   \n",
       "1           P2_CoT_FewShot                      200   \n",
       "2  P3_Persona_AntiExamples                      200   \n",
       "\n",
       "   Accuracy (on Valid Predictions)  JSON Validity Rate  \\\n",
       "0                            0.640                 1.0   \n",
       "1                            0.675                 1.0   \n",
       "2                            0.630                 1.0   \n",
       "\n",
       "   Reliability (Std Dev of Error)  \n",
       "0                        0.515464  \n",
       "1                        0.505647  \n",
       "2                        0.549097  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "print(\"--- RAW RESULTS DUMP ---\")\n",
    "if 'comparison_df' in locals():\n",
    "    print(\"Comparison Metrics:\")\n",
    "    display(comparison_df)\n",
    "else:\n",
    "    print(\"Error: comparison_df not found. Please ensure Cell 4 ran successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8929275,
     "sourceId": 14016446,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
