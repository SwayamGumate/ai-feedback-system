{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14016446,"sourceType":"datasetVersion","datasetId":8929275}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport json\nimport pandas as pd\nimport numpy as np\nimport time\nimport warnings\nfrom openai import OpenAI\nfrom tqdm import tqdm\nfrom IPython.display import display\n\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n# 1. Set the OpenRouter API Key and Base URL\nOPENROUTER_API_KEY = \"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\"\nOPENROUTER_BASE_URL = \"https://openrouter.ai/api/v1\"\n\n# 2. Define the OPENROUTER Client\nclient = OpenAI(\n    base_url=OPENROUTER_BASE_URL,\n    api_key=OPENROUTER_API_KEY\n)\n\n# 3. USE MISTRAL 7B WITHOUT THE ':free' SUFFIX\n# This should work reliably once your quota/credits are available.\nLLM_MODEL = \"mistralai/mistral-7b-instruct\" \n\nprint(f\"âœ… Setup complete. OpenRouter Client initialized using stable model: {LLM_MODEL}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-05T23:57:23.925661Z","iopub.execute_input":"2025-12-05T23:57:23.926001Z","iopub.status.idle":"2025-12-05T23:57:23.997457Z","shell.execute_reply.started":"2025-12-05T23:57:23.925978Z","shell.execute_reply":"2025-12-05T23:57:23.996394Z"}},"outputs":[{"name":"stdout","text":"âœ… Setup complete. OpenRouter Client initialized using stable model: mistralai/mistral-7b-instruct\n","output_type":"stream"}],"execution_count":111},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport warnings\nfrom tqdm import tqdm\n\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\n# âœ… CONFIRMED CORRECT FILE PATH from the diagnostic:\nFILE_PATH = '/kaggle/input/yelp-file/yelp.csv' \nSAMPLE_SIZE = 200\n\ndf_sample = pd.DataFrame() \n\ndef load_data_robustly(file_path):\n    \"\"\"Attempts to load data using comma, then tab, then pipe, then semicolon delimiters.\"\"\"\n    delimiters_to_try = [',', '\\t', '|', ';']\n    df = None\n    \n    print(f\"Attempting to load data at: {file_path}...\")\n\n    for sep in delimiters_to_try:\n        try:\n            print(f\"  -> Trying delimiter: '{sep}'\")\n            # Using on_bad_lines='skip' handles corrupt lines\n            df = pd.read_csv(file_path, sep=sep, on_bad_lines='skip', encoding='utf-8')\n            \n            # Check for reasonable column count and rows\n            if len(df.columns) > 1 and not df.empty:\n                return df, sep\n            \n        except Exception:\n            pass # Silently continue if loading fails\n\n    return None, None\n\ntry:\n    df_full, successful_sep = load_data_robustly(FILE_PATH)\n    \n    if df_full is None:\n        raise ValueError(\"Failed to load DataFrame with all attempted delimiters. Check file integrity.\")\n\n    print(f\"âœ… Data loaded successfully using delimiter: '{successful_sep}'\")\n    \n    # Check for required columns ('text' and 'stars' are the standard names)\n    if 'text' in df_full.columns and 'stars' in df_full.columns:\n        # Sample and rename the columns for the prediction phase\n        df_sample = df_full[['text', 'stars']].sample(n=SAMPLE_SIZE, random_state=42).reset_index(drop=True)\n        df_sample.rename(columns={'stars': 'actual_stars'}, inplace=True)\n        \n        print(f\"Total rows: {len(df_full)} | Sampled rows: {len(df_sample)}\")\n        print(\"\\nSampled Data Head (Ready for prediction):\")\n        print(df_sample.head())\n\n    else:\n        print(\"âŒ ERROR: Required columns ('text' and 'stars') not found after loading.\")\n        print(\"Available columns:\", df_full.columns.tolist())\n        \nexcept Exception as e:\n    print(f\"âŒ FATAL ERROR: Data loading failed. Error: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T23:57:23.999118Z","iopub.execute_input":"2025-12-05T23:57:23.999363Z","iopub.status.idle":"2025-12-05T23:57:24.118155Z","shell.execute_reply.started":"2025-12-05T23:57:23.999344Z","shell.execute_reply":"2025-12-05T23:57:24.117072Z"}},"outputs":[{"name":"stdout","text":"Attempting to load data at: /kaggle/input/yelp-file/yelp.csv...\n  -> Trying delimiter: ','\nâœ… Data loaded successfully using delimiter: ','\nTotal rows: 10000 | Sampled rows: 200\n\nSampled Data Head (Ready for prediction):\n                                                text  actual_stars\n0  We got here around midnight last Friday... the...             4\n1  Brought a friend from Louisiana here.  She say...             5\n2  Every friday, my dad and I eat here. We order ...             3\n3  My husband and I were really, really disappoin...             1\n4  Love this place!  Was in phoenix 3 weeks for w...             5\n","output_type":"stream"}],"execution_count":112},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport warnings\nfrom tqdm import tqdm\n\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\n# âœ… CONFIRMED CORRECT FILE PATH:\nFILE_PATH = '/kaggle/input/yelp-file/yelp.csv' \nSAMPLE_SIZE = 200\n\ndf_sample = pd.DataFrame() \n\ndef load_data_robustly(file_path):\n    \"\"\"Attempts to load data using comma, then tab, then pipe, then semicolon delimiters.\"\"\"\n    delimiters_to_try = [',', '\\t', '|', ';'] # Added semicolon\n    df = None\n    \n    print(f\"Attempting to load data at: {file_path}...\")\n\n    for sep in delimiters_to_try:\n        try:\n            print(f\"  -> Trying delimiter: '{sep}'\")\n            # Using on_bad_lines='skip' handles corrupt lines\n            df = pd.read_csv(file_path, sep=sep, on_bad_lines='skip', encoding='utf-8')\n            \n            # Check for reasonable column count and rows\n            if len(df.columns) > 1 and not df.empty:\n                return df, sep\n            \n        except Exception:\n            pass # Silently continue if loading fails\n\n    return None, None\n\ntry:\n    df_full, successful_sep = load_data_robustly(FILE_PATH)\n    \n    if df_full is None:\n        raise ValueError(\"Failed to load DataFrame with all attempted delimiters. Please check file integrity.\")\n\n    print(f\"âœ… Data loaded successfully using delimiter: '{successful_sep}'\")\n    \n    # Check for required columns ('text' and 'stars' are the standard names)\n    if 'text' in df_full.columns and 'stars' in df_full.columns:\n        # Sample and rename the columns for the prediction phase\n        df_sample = df_full[['text', 'stars']].sample(n=SAMPLE_SIZE, random_state=42).reset_index(drop=True)\n        df_sample.rename(columns={'stars': 'actual_stars'}, inplace=True)\n        \n        print(f\"Total rows: {len(df_full)} | Sampled rows: {len(df_sample)}\")\n        print(\"\\nSampled Data Head (Ready for prediction):\")\n        print(df_sample.head())\n\n    else:\n        # This handles the case where the data loaded but the columns are named differently (e.g., 'Review' and 'Rating')\n        print(\"âŒ ERROR: Required columns ('text' and 'stars') not found after loading.\")\n        print(\"Available columns:\", df_full.columns.tolist())\n        print(\"You must rename the correct columns (e.g., df_full.rename(columns={'Review': 'text', 'Rating': 'stars'}, inplace=True))\")\n        \nexcept Exception as e:\n    print(f\"âŒ FATAL ERROR: Data loading failed. Error: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T23:57:24.119064Z","iopub.execute_input":"2025-12-05T23:57:24.119357Z","iopub.status.idle":"2025-12-05T23:57:24.238342Z","shell.execute_reply.started":"2025-12-05T23:57:24.119327Z","shell.execute_reply":"2025-12-05T23:57:24.237416Z"}},"outputs":[{"name":"stdout","text":"Attempting to load data at: /kaggle/input/yelp-file/yelp.csv...\n  -> Trying delimiter: ','\nâœ… Data loaded successfully using delimiter: ','\nTotal rows: 10000 | Sampled rows: 200\n\nSampled Data Head (Ready for prediction):\n                                                text  actual_stars\n0  We got here around midnight last Friday... the...             4\n1  Brought a friend from Louisiana here.  She say...             5\n2  Every friday, my dad and I eat here. We order ...             3\n3  My husband and I were really, really disappoin...             1\n4  Love this place!  Was in phoenix 3 weeks for w...             5\n","output_type":"stream"}],"execution_count":113},{"cell_type":"code","source":"# --- 2. Main Processing Function (OpenRouter Implementation) ---\ndef predict_rating(review_text, prompt_function, strategy_name):\n    \"\"\"Sends a single review to the OpenRouter API and processes the response.\"\"\"\n    \n    prompt = prompt_function(review_text)\n    \n    try:\n        # Use ChatCompletion with strict JSON mode enabled\n        response = client.chat.completions.create(\n            model=LLM_MODEL,\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a Yelp review rating model. Respond strictly in the required JSON format.\"},\n                {\"role\": \"user\", \"content\": prompt}\n            ],\n            response_format={\"type\": \"json_object\"},\n            temperature=0.0\n        )\n\n        json_str = response.choices[0].message.content.strip()\n\n        try:\n            result = json.loads(json_str)\n            \n            # ðŸš¨ FIX: Model outputs 'rating', so we check for both 'rating' and 'predicted_stars'\n            pred_stars = result.get('rating') or result.get('predicted_stars')\n            explanation = result.get('explanation', '')\n            \n            # If the model included the review text, extract the rating from that output.\n            if pred_stars is None and 'review' in result and 'rating' in result:\n                 pred_stars = result.get('rating') \n\n\n            if isinstance(pred_stars, int) and 1 <= pred_stars <= 5:\n                return {\n                    'predicted_stars': pred_stars,\n                    'explanation': explanation,\n                    'json_valid': True\n                }\n            else:\n                # If the key was found but the value was not an int 1-5\n                return {\n                    'predicted_stars': np.nan,\n                    'explanation': json_str,\n                    'json_valid': True\n                }\n\n        except json.JSONDecodeError:\n            # If the output wasn't a valid JSON\n            return {\n                'predicted_stars': np.nan,\n                'explanation': json_str,\n                'json_valid': False\n            }\n\n    except Exception as e:\n        # API Error (Rate Limit 429, etc.)\n        return {\n            'predicted_stars': np.nan,\n            'explanation': f\"API_ERROR: {e}\",\n            'json_valid': False\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T23:57:24.240769Z","iopub.execute_input":"2025-12-05T23:57:24.241117Z","iopub.status.idle":"2025-12-05T23:57:24.250142Z","shell.execute_reply.started":"2025-12-05T23:57:24.241097Z","shell.execute_reply":"2025-12-05T23:57:24.249083Z"}},"outputs":[],"execution_count":114},{"cell_type":"code","source":"# --- Define Strategies to Run ---\nstrategies = [\n    {'name': 'P1_Base_Prompt', 'func': get_prompt_p1},\n    {'name': 'P2_CoT_FewShot', 'func': get_prompt_p2},\n    {'name': 'P3_Persona_AntiExamples', 'func': get_prompt_p3},\n]\n\n# Create a DataFrame to store all results (copies df_sample from Cell 2)\n# This assumes df_sample was successfully created in the previous step.\nresults_df = df_sample.copy()\nSAMPLE_SIZE = len(results_df)\n\nprint(f\"--- Starting Rating Prediction for {SAMPLE_SIZE} reviews ---\")\n\n# --- RATE LIMITING CONFIGURATION ---\nDELAY_SECONDS = 0.5 \nREQUESTS_BEFORE_DELAY = 10 \n# -------------------------------------\n\n# Iterate through each prompting strategy\nfor strategy in strategies:\n    strategy_name = strategy['name']\n    prompt_func = strategy['func']\n\n    print(f\"\\nProcessing Strategy: **{strategy_name}**\")\n\n    predictions = []\n\n    # Use tqdm for a progress bar while iterating through the reviews\n    for i, review_text in enumerate(tqdm(results_df['text'], desc=f\"   {strategy_name}\")):\n\n        # Implement Delay Check\n        if i > 0 and i % REQUESTS_BEFORE_DELAY == 0:\n            time.sleep(DELAY_SECONDS)\n\n        # Get prediction and metadata\n        result = predict_rating(review_text, prompt_func, strategy_name)\n        predictions.append(result)\n\n    # Convert results to a temporary DataFrame and add to main DataFrame\n    temp_df = pd.DataFrame(predictions)\n    results_df[f'{strategy_name}_pred'] = temp_df['predicted_stars'].values\n    results_df[f'{strategy_name}_valid'] = temp_df['json_valid'].values\n    results_df[f'{strategy_name}_explanation'] = temp_df['explanation'].values\n\n\nprint(\"\\n--- All predictions complete ---\")\nprint(\"\\nSample of results_df (Actual vs Predictions):\")\nprint(results_df[[\n    'actual_stars', 'P1_Base_Prompt_pred', 'P2_CoT_FewShot_pred', 'P3_Persona_AntiExamples_pred'\n]].head())\n\n# --- Evaluation ---\nevaluation_metrics = []\n\nfor strategy in strategies:\n    strategy_name = strategy['name']\n    pred_col = f'{strategy_name}_pred'\n    \n    # Calculate Accuracy (on valid predictions)\n    correct_predictions = np.sum(np.isclose(results_df['actual_stars'], results_df[pred_col]))\n    valid_pred_count = results_df[pred_col].count()\n    accuracy = correct_predictions / valid_pred_count if valid_pred_count > 0 else 0\n    \n    # Calculate JSON Validity Rate\n    json_valid_count = results_df[f'{strategy_name}_valid'].sum()\n    validity_rate = json_valid_count / SAMPLE_SIZE\n    \n    # Calculate Reliability (Standard Deviation of Absolute Error)\n    error = np.abs(results_df['actual_stars'] - results_df[pred_col])\n    consistency_std = error[~error.isna()].std()\n    \n    evaluation_metrics.append({\n        'Approach': strategy_name,\n        'Total Valid Predictions': valid_pred_count,\n        'Accuracy (on Valid Predictions)': accuracy,\n        'JSON Validity Rate': validity_rate,\n        'Reliability (Std Dev of Error)': consistency_std\n    })\n\n# Convert metrics to a final comparison table\ncomparison_df = pd.DataFrame(evaluation_metrics)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T23:57:24.251037Z","iopub.execute_input":"2025-12-05T23:57:24.251291Z","iopub.status.idle":"2025-12-06T00:25:05.797904Z","shell.execute_reply.started":"2025-12-05T23:57:24.251272Z","shell.execute_reply":"2025-12-06T00:25:05.796370Z"}},"outputs":[{"name":"stdout","text":"--- Starting Rating Prediction for 200 reviews ---\n\nProcessing Strategy: **P1_Base_Prompt**\n","output_type":"stream"},{"name":"stderr","text":"   P1_Base_Prompt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [07:42<00:00,  2.31s/it]\n","output_type":"stream"},{"name":"stdout","text":"\nProcessing Strategy: **P2_CoT_FewShot**\n","output_type":"stream"},{"name":"stderr","text":"   P2_CoT_FewShot: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [15:23<00:00,  4.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"\nProcessing Strategy: **P3_Persona_AntiExamples**\n","output_type":"stream"},{"name":"stderr","text":"   P3_Persona_AntiExamples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [04:35<00:00,  1.38s/it]","output_type":"stream"},{"name":"stdout","text":"\n--- All predictions complete ---\n\nSample of results_df (Actual vs Predictions):\n   actual_stars  P1_Base_Prompt_pred  P2_CoT_FewShot_pred  \\\n0             4                    4                    4   \n1             5                    5                    5   \n2             3                    3                    4   \n3             1                    1                    1   \n4             5                    5                    5   \n\n   P3_Persona_AntiExamples_pred  \n0                             4  \n1                             5  \n2                             4  \n3                             1  \n4                             5  \n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":115},{"cell_type":"code","source":"# --- RUN THIS CODE NOW (Cell 5) ---\nfrom IPython.display import display\n\nprint(\"\\n\\n# ðŸ“Š Evaluation Results: Prompting Strategy Comparison\\n\")\n\n# Display the main comparison table\nprint(\"--- Performance Metrics Table ---\")\n# The comparison_df creation failed because all predictions were NaN, but let's try the diagnostic check.\n    \nprint(\"\\n--- Diagnostic Check (First Invalid Responses) ---\")\n# Check the raw output for the first failure in P2 (as an example)\ntry:\n    # We explicitly check for the NaN values in the prediction column\n    invalid_p2 = results_df[results_df['P2_CoT_FewShot_pred'].isna()].head(1)\n    \n    if not invalid_p2.empty:\n        print(\"\\nExample P2 Failure (Raw Model Output):\")\n        # Find the index of the first failure for clear display\n        for index, row in invalid_p2.iterrows():\n            print(f\"Actual Star Rating: {row['actual_stars']}\")\n            print(\"----------------------------------------\")\n            # This is the key: the raw model output that caused the NaN\n            print(f\"RAW MODEL OUTPUT: {row['P2_CoT_FewShot_explanation']}\")\n    else:\n        print(\"P2 diagnostic check: All predictions were valid (1-5 integer).\")\n        \nexcept NameError:\n    print(\"âŒ ERROR: 'results_df' is not defined. Please ensure Cell 4 completed successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T00:25:20.028101Z","iopub.execute_input":"2025-12-06T00:25:20.028644Z","iopub.status.idle":"2025-12-06T00:25:20.043339Z","shell.execute_reply.started":"2025-12-06T00:25:20.028600Z","shell.execute_reply":"2025-12-06T00:25:20.042308Z"}},"outputs":[{"name":"stdout","text":"\n\n# ðŸ“Š Evaluation Results: Prompting Strategy Comparison\n\n--- Performance Metrics Table ---\n\n--- Diagnostic Check (First Invalid Responses) ---\nP2 diagnostic check: All predictions were valid (1-5 integer).\n","output_type":"stream"}],"execution_count":117},{"cell_type":"code","source":"import pandas as pd\nfrom IPython.display import display\nprint(\"--- RAW RESULTS DUMP ---\")\nif 'comparison_df' in locals():\n    print(\"Comparison Metrics:\")\n    display(comparison_df)\nelse:\n    print(\"Error: comparison_df not found. Please ensure Cell 4 ran successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T00:31:14.071020Z","iopub.execute_input":"2025-12-06T00:31:14.071337Z","iopub.status.idle":"2025-12-06T00:31:14.084968Z","shell.execute_reply.started":"2025-12-06T00:31:14.071312Z","shell.execute_reply":"2025-12-06T00:31:14.083250Z"}},"outputs":[{"name":"stdout","text":"--- RAW RESULTS DUMP ---\nComparison Metrics:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                  Approach  Total Valid Predictions  \\\n0           P1_Base_Prompt                      200   \n1           P2_CoT_FewShot                      200   \n2  P3_Persona_AntiExamples                      200   \n\n   Accuracy (on Valid Predictions)  JSON Validity Rate  \\\n0                            0.640                 1.0   \n1                            0.675                 1.0   \n2                            0.630                 1.0   \n\n   Reliability (Std Dev of Error)  \n0                        0.515464  \n1                        0.505647  \n2                        0.549097  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Approach</th>\n      <th>Total Valid Predictions</th>\n      <th>Accuracy (on Valid Predictions)</th>\n      <th>JSON Validity Rate</th>\n      <th>Reliability (Std Dev of Error)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>P1_Base_Prompt</td>\n      <td>200</td>\n      <td>0.640</td>\n      <td>1.0</td>\n      <td>0.515464</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>P2_CoT_FewShot</td>\n      <td>200</td>\n      <td>0.675</td>\n      <td>1.0</td>\n      <td>0.505647</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>P3_Persona_AntiExamples</td>\n      <td>200</td>\n      <td>0.630</td>\n      <td>1.0</td>\n      <td>0.549097</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":121}]}